from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import json
import re
from dotenv import load_dotenv
import os

load_dotenv()  # ğŸ”¥ .env íŒŒì¼ ë¡œë“œ

# âœ… Gemini ì„¤ì •
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel(os.getenv("GEMINI_MODEL"))

# âœ… FastAPI ì•±
app = FastAPI()

# âœ… CORS í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… ìš”ì²­ ëª¨ë¸
class TextRequest(BaseModel):
    text: str

class CommentRequest(BaseModel):
    comment: str

class UrlRequest(BaseModel):
    url: str

class AnalyzeFullRequest(BaseModel):
    url: str
    comment: str

@app.post("/analyze-comment-full")
def analyze_comment_full(data: AnalyzeFullRequest):
    url = data.url.strip()
    comment = data.comment.strip()

    # 1. í˜ì´ì§€ ìš”ì•½
    context_summary = summarize_with_url(url)

    # 2. ëŒ“ê¸€ ë¶„ì„ (í•µì‹¬ ì£¼ì¥ + ë°˜ëŒ€ ì‹œì„ )
    analysis_prompt = f"""
ë„ˆëŠ” ì¤‘ë¦½ì ì´ê³  ê· í˜• ì¡íŒ ì¸ê³µì§€ëŠ¥ì´ë‹¤. ë‹¤ìŒ ë¬¸ì¥ì„ ì½ê³ :
1. í•µì‹¬ ì£¼ì¥(opinion)ì„ ìš”ì•½í•˜ê³ ,
2. ë°˜ëŒ€ ì‹œì„ (opposition)ì„ í•¨ê»˜ ì œì‹œí•´.

ë¬¸ì¥: "{comment}"
[ì°¸ê³  ìš”ì•½]: {context_summary}

ê²°ê³¼ëŠ” ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜:
{{
  "opinion": "...",
  "opposition": "..."
}}
"""
    parsed_analysis = safe_extract_json(model.generate_content(analysis_prompt).text, "ë¶„ì„")

    # 3. ë‰´ìŠ¤ í‚¤ì›Œë“œ ìƒì„± ë° ë§í¬ ë°˜í™˜
    news_prompt = f"""
ë‹¤ìŒ ë¬¸ì¥ì„ ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë¡œ ë°”ê¿”ì¤˜. í•œêµ­ ì‚¬íšŒ ì´ìŠˆ ìœ„ì£¼ë¡œ ì§§ê³  ëª…í™•í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ê³ ,
JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì¤˜. ì˜ˆì‹œ: {{ "query": "ì—¬ì„± ì§•ë³‘ì œ ë…¼ë€" }}

ë¬¸ì¥: "{comment}"
[ì°¸ê³  ìš”ì•½]: {context_summary}

ì£¼ì˜: ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´ì¤˜.
"""
    query_obj = safe_extract_json(model.generate_content(news_prompt).text, "ë‰´ìŠ¤")
    query = query_obj.get("query", "")
    news = [
        {
            "title": f"ğŸ” ê´€ë ¨ ë‰´ìŠ¤: {query}",
            "url": f"https://www.google.com/search?q={query}&tbm=nws"
        }
    ] if query else [
        {
            "title": "ğŸ” ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ë³´ê¸°",
            "url": f"https://www.google.com/search?q={comment}&tbm=nws"
        }
    ]

    return {
        "summary": context_summary,
        "opinion": parsed_analysis.get("opinion"),
        "opposition": parsed_analysis.get("opposition"),
        "news": news
    }

def summarize_with_url(url: str) -> str:
    if any(bad in url for bad in ["google.com/search", "search.naver.com", "youtube.com/results"]):
        return "ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if "youtube.com/watch" in url or "youtu.be/" in url:
        return summarize_youtube(url).get("summary", "")
    return summarize_url(url).get("summary", "")

def summarize_url(url: str) -> Dict[str, str]:
    try:
        headers = { "User-Agent": "Mozilla/5.0" }
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")

        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        blocks = soup.find_all(["p", "div", "section", "article"])
        text_chunks = [block.get_text(separator=" ", strip=True) for block in blocks if len(block.get_text(strip=True)) >= 100]
        text = re.sub(r'\s+', ' ', " ".join(text_chunks))

        if len(text) < 200:
            return {"summary": "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        prompt = f"""
ë‹¤ìŒì€ ì›¹ í˜ì´ì§€ ë³¸ë¬¸ì…ë‹ˆë‹¤. ê´‘ê³ ë‚˜ ë°˜ë³µ ë¬¸êµ¬ëŠ” ë¬´ì‹œí•˜ê³  í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜:

{text[:4000]}
"""
        response = model.generate_content(prompt)
        return {"summary": response.text.strip()}

    except Exception as e:
        print("âŒ í˜ì´ì§€ ìš”ì•½ ì˜¤ë¥˜:", e)
        return {"summary": "í˜ì´ì§€ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

def summarize_youtube(youtube_url: str) -> Dict[str, str]:
    try:
        print("ğŸ”¥ [ìœ íŠœë¸Œ ìš”ì•½ ì‹œì‘] URL:", youtube_url)
        oembed_url = f"https://www.youtube.com/oembed?url={youtube_url}&format=json"
        res = requests.get(oembed_url)
        if res.status_code != 200:
            print("âŒ oEmbed ì‘ë‹µ ì‹¤íŒ¨:", res.status_code)
            return {"summary": "ìœ íŠœë¸Œ ì˜ìƒ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        data = res.json()
        title = data.get("title", "").strip()
        if not title:
            return {"summary": "ì œëª© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}

        prompt = f"""
ë‹¤ìŒì€ ìœ íŠœë¸Œ ì˜ìƒì˜ ì œëª©ì…ë‹ˆë‹¤. ì˜ìƒì˜ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ ì¤‘ë¦½ì ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.

ì œëª©: {title}
"""
        response = model.generate_content(prompt)
        return {"summary": response.text.strip()}

    except Exception as e:
        print("âŒ ìœ íŠœë¸Œ ìš”ì•½ ì˜¤ë¥˜:", e)
        return {"summary": "ìœ íŠœë¸Œ ì˜ìƒ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

def safe_extract_json(text: str, context: str = "") -> Dict[str, str]:
    try:
        match = re.search(r'\{[\s\S]*?\}', text)
        return json.loads(match.group()) if match else {}
    except Exception as e:
        print(f"âŒ {context} JSON íŒŒì‹± ì˜¤ë¥˜:", e)
        return {}


